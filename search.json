[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Estadístico: un enfoque práctico con R",
    "section": "",
    "text": "Prefacio\nEn este “Quarto Book” se ha integrado parte del material para la asignatura Aprendizaje Estadísitico y Otras Técnicas Analíticas Avanzadas del Máster MUMADE. Concretamente la parte de Aprendizaje Estadístico.\nEl material, con un enfoque totalmente práctico/aplicado, es una traducción/adaptación de los “Labs” del libro An Introduction to Statistical Learning, with applications in R, que está accesible libremente en formato PDF en https://www.statlearning.com. También está accesible en https://www.statlearning.com/resources-second-edition numeroso material asociado al libro: slides, los citados Labs originales (en formato .Rmd y .html), los scripts de R de los Labs, etc. Aquí aprovecho dicho material que los autores ponen a nuestra disposición para fines educativos.\nEn el proceso de traducción/adaptación me he tomado la licencia de omitir algunas partes, recortar otras (principalmente por duplicidades), renombrar secciones y crear un anexo con elementos transversales que aparecían en el material original.\n\nUn comentario importante: en la adaptación de los Labs originales he omitido intencionadamente la función attach(), cuyo uso desaconsejo, como muchos expertos de R (pude verse una discusión sobre ello aquí)\n\nUn par de buenas alternativas (en castellano) a este material (para profundizar y ampliar contenidos, consultar detalles u otros enfoques) son:\n\nEl libro Métodos predictivos de aprendizaje estadístico. Por el nombre ya queda claro la similitud con este material, y el contenido del libro es más extenso y profundo que lo que se presenta en este material. En algún Capítulo/Técnica el libro utiliza otro enfoque y otras funciones de R. Asociado con el libro se puede encontrar el paquete de R mpae que incluye funciones y datos utilizados en el libro.\nEl libro Fundamentos de ciencia de datos con R. El nombre no refleja tan claramente la similitud, pero, entre el numeroso contenido se pueden encontrar todas las técnicas que se presentan en este material. También tiene asociado un paquete de R: CDR, que contiene los conjuntos de datos que aparecen en el libro y no están disponibles en otros paquetes.\n."
  },
  {
    "objectID": "Lab1-intro.html",
    "href": "Lab1-intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Material original\n\n\n\n\n\n\nTipo\n\n\n\n\n\nSlides:\nhttps://hastie.su.domains/ISLR2/Slides/Ch1_Inroduction.pdf\n\n\n\n\nNota Víctor: Este tema del libro, no tiene Lab asociado.\n\nLas slides indicadas arriba se dedican a mostrar algunos problemas que pueden resolverse utilizando Aprendizaje Estadístico y a mencionar los grandes términos que se manejan en este campo: Aprendizaje supervisado, regresión, clasificación, datos de entrenamiento (con ellos se “aprende”), Aprendizaje no supervisado… Y la “diferencia” entre Statistical Learning y Machine Learning!\nEs recomendable ver el vídeo de dichas slides, realizado por los dos autores principales del libro: Trevor Hastie and Robert Tibshirani, https://www.youtube.com/watch?v=5N9V07EIfIg&t=39s"
  },
  {
    "objectID": "Lab2-AE.html#datos-de-entrenamiento-y-test",
    "href": "Lab2-AE.html#datos-de-entrenamiento-y-test",
    "title": "2  Aprendizaje Estadístico",
    "section": "2.1 Datos de entrenamiento y test",
    "text": "2.1 Datos de entrenamiento y test\nEl nombre de Aprendizaje viene del hecho de tomar un conjunto de datos y utilizarlos como datos de entrenamiento, en inglés train, para que “el modelo entrene/aprenda”. Una vez determinado/especificado el modelo, basado en los datos de entrenamiento, se evalúa su rendimiento/precisión utilizando otro conjunto de datos denominado de test, de prueba o de validación (a lo largo del material se irán utilizando indistintamente dichos términos).\nExisten distintas maneras de medir el rendimiento/precisión de los modelos. En cada uno de los siguientes capítulos se especificará la medida o medidas que se utilizan para ello, que de modo genérico se denomina medida o tasa de error. La intuición indica, y la práctica lo confirma, que la medida/tasa de error de entrenamiento suele ser demasiado optimista: tiende a subestimar la medida/tasa de error de test. Ésta última es una medida/tasa de error más realista, en el sentido de que en la práctica estaremos interesados en el rendimiento de nuestro modelo, no en los datos que usamos para ajustar el modelo, sino en los posibles nuevos datos que se obtengan.\nPara poner en práctica este enfoque se suele dividir el conjunto de datos disponible en dichos subconjuntos de entrenamiento y test, completamente separados (sin ningún elemento en común). Es habitual tomar como datos de entrenamiento entre un 80% y un 90%, considerando el resto como datos de test. En el anexo Herramientas se detallan diversas maneras de obtener dichos conjuntos, en función de los datos/variables disponibles."
  },
  {
    "objectID": "Lab2-AE.html#técnicas-de-ae",
    "href": "Lab2-AE.html#técnicas-de-ae",
    "title": "2  Aprendizaje Estadístico",
    "section": "2.2 Técnicas de AE",
    "text": "2.2 Técnicas de AE\nNo es para nada un resumen, pero la figura de más abajo, tomada de las slides originales, sirve para ver distintos métodos de modelización estadística que vamos a ver en este material y “ubicarlos” en esas dos dimensiones de interpretabilidad y flexibilidad."
  },
  {
    "objectID": "Lab3-RegLin.html#regresión-lineal-simple",
    "href": "Lab3-RegLin.html#regresión-lineal-simple",
    "title": "3  Lab3 Regresión lineal",
    "section": "3.1 Regresión lineal simple",
    "text": "3.1 Regresión lineal simple\nEl paquete ISLR2 contiene el conjunto de datos Boston, que registra medv (“median value”: valor mediano de las casas ocupadas por sus propietarios, en $1000s) para 506 distritos censales de Boston. Buscaremos predecir medv utilizando 12 predictores como rm (“room mean”: número promedio de habitaciones por casa), age (“casas construidas antes de 1940, en %”!!) y lstat (“lower status”: población de menor estatus -nivel socioeconómico bajo-, en %).\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nPara obtener más información sobre el conjunto de datos, podemos escribir ?Boston.\n\n3.1.1 lm() (ajustar modelos)\nComenzaremos usando la función lm() para ajustar un modelo de regresión lineal simple, con medv como variable respuesta y lstat como variable predictora. La sintaxis básica es lm(y ~ x, data), donde y es la respuesta, x es el predictor y data es el conjunto de datos en el que están las dos variables.\n\nlm.fit <- lm(medv ~ lstat, data = Boston)\n\nNota: Al proporcionar data = Boston, se le dice a R que las variables medv y lstat están en Boston. Si no se le proporciona el valor de data R busca en el Environment, y si allí no están las variables muestra un error.\nSi escribimos lm.fit, se muestra información básica sobre el modelo: coeficientes.\nPara obtener información más detallada: summary(lm.fit) que nos devuelve p-valores y errores estándar para los coeficientes, así como el estadístico \\(R^2\\) y el estadístico \\(F\\) para el modelo.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\nNota Víctor: El material original no comenta algo fundamental de la salida anterior… ¿cómo se interpretan los coeficientes?, ¿p-valores?… ¿\\(R^2\\)?… En el Capítulo Modelización lineal del libro Fundamentos de ciencia de datos con R (“CDR”) se puede encontrar las respuestas de las preguntas anteriores.\n\nPodemos usar la función names() para averiguar qué otra información se almacena en lm.fit.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nAunque podemos extraer dicha información por su nombre —p.ej. lm.fit$coefficients — es más seguro usar las funciones de extracción como coef() para acceder a ellas.\n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nPara obtener el intervalo de confianza para las estimaciones de los coeficientes, podemos usar la función confint().\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\n\n\n3.1.2 Predicciones\nLa función predict() se puede utilizar para producir intervalos de confianza e intervalos de predicción para la predicción de medv para un valor dado de lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nDe estas salidas se obtiene, que el intervalo de confianza del 95% asociado con un valor lstat de 10 es (24.47, 25.63), y el intervalo de predicción del 95% es (12.83, 37.28). Como era de esperar, los intervalos de confianza y de predicción se centran en torno al mismo punto (un valor predicho de 25.05 para medv cuando lstat es igual a 10), pero estos últimos son sustancialmente más amplios.\n\n\n3.1.3 Diagrama de dispersión\nAhora dibujaremos medv y lstat junto con la recta de regresión de mínimos cuadrados (obtenida con el ajuste lm.fit) usando las funciones plot() y abline().\n\npar(pty = \"s\") # sentencia añadida (no incluida en el código original)\n# \"p\"lot \"ty\"pe \"s\"quare (recomendado para gráficos de dispersión)\nplot(Boston$lstat, Boston$medv)\nabline(lm.fit, lwd = 3, col = \"red\") # a=intercept, b=slope\n\n\n\n\nHay alguna evidencia de no linealidad en la relación entre lstat y medv. Exploraremos este problema más adelante en esta práctica.\nLa función abline() se puede usar para dibujar cualquier recta, no solo la recta de regresión de mínimos cuadrados. Para dibujar una recta con intersección a y pendiente b, escribimos abline(a, b). El argumento lwd = 3 hace que el ancho de la recta de regresión aumente en un factor de 3; esto también funciona para las funciones plot() y lines(). También podemos usar la opción pch para crear diferentes símbolos de trazado.\n\npar(mfrow = c(1, 2), pty = \"s\") # gráficos en formato: 1 fila y 2 columnas\nplot(Boston$lstat, Boston$medv, col = \"red\", pch = 20)\nabline(lm.fit, lwd = 3)\n#plot(Boston$lstat, Boston$medv, pch = \"+\")\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\n\n\n3.1.4 Diagnóstico\nA continuación examinamos algunos diagramas de diagnóstico, varios de los cuales se discuten en la Sección 3.3.3 del libro. Al aplicar la función plot() a la salida de lm() se producen automáticamente 4 gráficos de diagnóstico.\nNota técnica: En la consola esta función producirá un gráfico, y al presionar ENTER se generará el siguiente gráfico, etc. Sin embargo, a menudo es conveniente ver los cuatro gráficos juntos. Podemos lograr esto usando las funciones par() y mfrow(), que le dicen a R que divida la pantalla de visualización en paneles separados para que se puedan ver múltiples gráficos simultáneamente. Por ejemplo, par(mfrow = c(2, 2)) divide la ventana de gráficos en una cuadrícula de paneles de 2x2.\n\npar(mfrow = c(2, 2),\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(lm.fit)\n\n\n\n\nAlternativamente, podemos calcular los residuos de un ajuste de regresión lineal usando la función residuals(). La función rstudent() devolverá los residuos studentizados, y podemos usar esta función para dibujar los residuos contra los valores ajustados.\n\npar(mfrow = c(1, 2),\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nSobre la base de las gráficas residuales, existe alguna evidencia de no linealidad. Los estadísticos de apalancamiento (Leverage) se pueden calcular para cualquier número de predictores usando la función hatvalues().\n\npar(mfrow = c(1, 1), pty = \"s\")\nplot(hatvalues(lm.fit))\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nLa función which.max() identifica el índice del elemento más grande de un vector. En este caso, nos dice qué observación tiene el estadístico de apalancamiento más grande."
  },
  {
    "objectID": "Lab3-RegLin.html#regresión-lineal-múltiple",
    "href": "Lab3-RegLin.html#regresión-lineal-múltiple",
    "title": "3  Lab3 Regresión lineal",
    "section": "3.2 Regresión lineal múltiple",
    "text": "3.2 Regresión lineal múltiple\nPara ajustar un modelo de regresión lineal múltiple usando mínimos cuadrados, nuevamente usamos la función lm(). La sintaxis lm(y ~ x1 + x2 + x3) se usa para ajustar un modelo con tres predictores, x1, x2 y x3. La función summary() mostrará los coeficientes de regresión para todos los predictores considerados.\n\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\n3.2.1 Modelo con todos los predictores\nEl conjunto de datos Boston contiene 12 variables, por lo que sería engorroso tener que escribirlas todas para realizar una regresión usando todos los predictores. En su lugar, podemos usar la siguiente abreviatura:\n\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  < 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: < 2.2e-16\n\n\n\nNota Víctor: De nuevo, el material original no interpreta la salida anterior donde hay coeficientes positivos y negativos, ¡significativos y no significativos! ¿\\(R^2\\)? ¿significación global del modelo?… (véase el citado Capítulo Modelización lineal del libro “CDR”).\n\nComo antes, se puede acceder por nombre a los componentes individuales, ahora del objeto de resumen (escriba ?summary.lm para ver qué hay disponible). Así\n\nsummary(lm.fit)$r.sq nos da el \\(R^2\\), y\nsummary(lm.fit)$sigma nos da el Residual standard error.\n\n\n\n3.2.2 vif()\nLa función vif(), del paquete car, se puede utilizar para calcular los factores de inflación de la varianza. El paquete car no forma parte de la instalación básica de R, por lo que debe instalarse para poder usarlo (véase el Anexo Herramientas).\nPara estos datos, la mayoría de los VIF son bajos o moderados.\n\nlibrary(car)\n\nCargando paquete requerido: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 3.954037 \n     rad      tax  ptratio    lstat \n7.445301 9.002158 1.797060 2.870777 \n\n\n\nNota Víctor: Regla de decisión: No preocuparse de la multicolinealidad si vif < 5, incluso vif < 10.\n\n\n\n3.2.3 Quitando predictores\n¿Qué pasa si se quiere realizar una regresión usando todas las variables menos una (o varias)? Por ejemplo, en el resultado de la regresión anterior, age tiene un p-valor alto. Entonces, es posible que deseemos ejecutar una regresión que excluya este predictor. La siguiente sintaxis da como resultado una regresión que usa todos los predictores excepto age.\n\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  < 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: < 2.2e-16\n\n\nAlternativamente, se puede usar la función update().\n\nlm.fit1 <- update(lm.fit, ~ . - age)\n\n\nNota Víctor: ¿Cómo quitar varios predictores?"
  },
  {
    "objectID": "Lab3-RegLin.html#términos-de-interacción",
    "href": "Lab3-RegLin.html#términos-de-interacción",
    "title": "3  Lab3 Regresión lineal",
    "section": "3.3 Términos de interacción",
    "text": "3.3 Términos de interacción\nEs fácil incluir términos de interacción en un modelo lineal utilizando la función lm(). La sintaxis lstat:black le dice a R que incluya un término de interacción entre lstat y black. La sintaxis lstat * age incluye simultáneamente lstat, age y el término de interacción lstat:age como predictores; es una abreviatura de lstat + age + lstat:age.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "Lab3-RegLin.html#transformaciones-no-lineales-de-los-predictores",
    "href": "Lab3-RegLin.html#transformaciones-no-lineales-de-los-predictores",
    "title": "3  Lab3 Regresión lineal",
    "section": "3.4 Transformaciones no lineales de los predictores",
    "text": "3.4 Transformaciones no lineales de los predictores\nLa función lm() también admite transformaciones no lineales de los predictores. Por ejemplo, dado un predictor X, podemos crear un predictor X^2 usando I(X^2) (véase el Anexo Herramientas). Realizamos la regresión de medv sobre lstat y lstat^2.\n\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2), data = Boston)\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nEl p-valor cercano a cero asociado con el término cuadrático sugiere que, su inclusión, conduce a un modelo mejorado.\n\n3.4.1 anova()\nUsamos la función anova() (véase el Anexo Herramientas) para cuantificar aún más hasta qué punto el ajuste cuadrático es superior al ajuste lineal.\n\nlm.fit <- lm(medv ~ lstat, data = Boston)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAquí, el Modelo 1 representa el “submodelo” lineal que contiene solo un predictor, lstat, es decir lm.fit, mientras que el Modelo 2 corresponde al modelo cuadrático, modelo “completo”, que tiene dos predictores, lstat y lstat^2, llamado lm.fit2. Aquí el estadístico \\(F\\) es 135 y el p-valor asociado es virtualmente cero. Esto proporciona una evidencia muy clara de que el modelo que contiene los predictores lstat y lstat^2 es muy superior al modelo que solo contiene el predictor lstat. Esto no es sorprendente, ya que anteriormente vimos evidencia de no linealidad en la relación entre medv y lstat. Si escribimos\n\npar(mfrow = c(2, 2),\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(lm.fit2)\n\n\n\n\nvemos que cuando el término lstat^2 se incluye en el modelo, hay un patrón poco perceptible en los residuos.\n\nNota Víctor: es lo deseable, que los residuos NO tengan estructura (sean ruido blanco)\n\n\n\n3.4.2 Ajuste polinomial\nPara crear un ajuste cúbico, podemos incluir un predictor de la forma I(X^3). Sin embargo, existe un mejor enfoque consistente en usar la función poly() para crear el polinomio dentro de lm() (véase el Anexo Herramientas). Por ejemplo, la siguiente sentencia produce un ajuste polinomial de quinto orden:\n\nlm.fit5 <- lm(medv ~ poly(lstat, 5), data = Boston)\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\n\nEsto sugiere que la inclusión de términos polinómicos adicionales, hasta el quinto orden, ¡conduce a una mejora en el ajuste del modelo! Sin embargo, una investigación más profunda de los datos revela que ningún término polinomial más allá del quinto orden tiene p-valores significativos en un ajuste de regresión.\nNota técnica: en el Anexo Herramientas se menciona que la función ploy() ortogonaliza los predictores por defecto. Sin embargo un modelo lineal aplicado a la salida de la función poly() tendrá los mismos valores ajustados que un modelo lineal aplicado a los polinomios sin procesar (al incluir en poly el argumento raw = TRUE), aunque las estimaciones de los coeficientes, los errores estándar y los p-valores serán diferentes.\n\n\n3.4.3 Transformación logarítmica\nDe ninguna manera estamos restringidos a usar sólo transformaciones polinómicas de los predictores. Aquí se realiza una transformación logarítmica.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "Lab3-RegLin.html#predictores-cualitativos",
    "href": "Lab3-RegLin.html#predictores-cualitativos",
    "title": "3  Lab3 Regresión lineal",
    "section": "3.5 Predictores cualitativos",
    "text": "3.5 Predictores cualitativos\nAhora examinaremos los datos Carseats, del paquete ISLR2. Intentaremos predecir Sales (ventas de asientos de seguridad para niños) en 4000 ubicaciones en función de una serie de predictores.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nEl conjunto de datos Carseats incluye predictores cualitativos como ShelveLoc, un indicador de la calidad de la ubicación de las estanterías, es decir, el espacio dentro de una tienda en el que se muestra el asiento para el automóvil. El predictor ShelveLoc toma tres valores posibles: Mala, Media y Buena (ubicación). Dada una variable cualitativa como ShelveLoc, R genera automáticamente variables ficticias, variables dummys. La función contrasts() devuelve la codificación que R usa para las variables ficticias (véase el Anexo Herramientas).\n\ncontrasts(Carseats$ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nA continuación ajustamos un modelo de regresión múltiple, que también incluye algunos términos de interacción.\n\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age,\n             data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\n\nEl hecho de que el coeficiente de ShelveLocGood en el resultado de la regresión sea positivo indica que una buena ubicación de las estanterías está asociada con ventas altas (en comparación con una mala ubicación).\n\nNota Víctor: el comentario coincide con la lógica, pero, además, a) se cuantifica la magnitud del “aumento”, y b) lo que es mejor, el “cambio” es significativo.\n\nY ShelveLocMedium tiene un coeficiente positivo más pequeño, lo que indica que una ubicación de estantería media está asociada con ventas más altas que una ubicación de estantería mala, pero ventas más bajas que una ubicación de estantería buena."
  },
  {
    "objectID": "Lab3-RegLin.html#escritura-de-funciones-movido",
    "href": "Lab3-RegLin.html#escritura-de-funciones-movido",
    "title": "3  Lab3 Regresión lineal",
    "section": "Escritura de funciones (movido)",
    "text": "Escritura de funciones (movido)\nEn el Lab3 original se dedica este último apartado (pequeño) a la creación/escritura de funciones. Como su contenido es transversal se ha movido al Anexo Herramientas."
  },
  {
    "objectID": "Lab4-Clasif.html#datos-smarket-mercado-de-valores",
    "href": "Lab4-Clasif.html#datos-smarket-mercado-de-valores",
    "title": "4  Lab4 Clasificación",
    "section": "Datos: Smarket, mercado de valores",
    "text": "Datos: Smarket, mercado de valores\nExaminamos algunos resúmenes numéricos y gráficos de los datos Smarket del paquete ISLR2. Este conjunto de datos consiste en rendimientos porcentuales para el índice bursátil S&P 500 durante 1.250 días, desde comienzos de 2001 hasta finales de 2005. Para cada fecha, se han registrado los rendimientos porcentuales de cada uno de los cinco días de negociación anteriores, desde lagone hasta lagfive. También se han registrado volume (el número de acciones negociadas el día anterior, en miles de millones), Today (el rendimiento porcentual en la fecha en cuestión) y direction (si el mercado estaba Up o Down en esta fecha). Nuestro objetivo es predecir direction (una variable respuesta cualitativa) usando las otras características.\n\nlibrary(ISLR2)\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npar(pty=\"s\")\npairs(Smarket)\n\n\n\n\nLa función cor() produce una matriz que contiene todas las correlaciones por pares entre los predictores en un conjunto de datos. El primer comando a continuación da un mensaje de error porque la variable direction es cualitativa.\n\ncor(Smarket)\n\nError in cor(Smarket): 'x' debe ser numérico\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n\n\nComo era de esperar, las correlaciones entre las variables lag... y los rendimientos de hoy (Today) son cercanas a 0. En otras palabras, parece haber poca correlación entre los rendimientos de hoy y los rendimientos de días anteriores. La única correlación sustancial es entre Year y volume. Al dibujar los datos, que están ordenados cronológicamente, vemos que volume aumenta con el tiempo. En otras palabras, el número promedio de acciones negociadas diariamente aumentó de 2001 a 2005.\n\nplot(Smarket$Volume, type = \"l\") # añadido el \"type\""
  },
  {
    "objectID": "Lab4-Clasif.html#regresión-logística",
    "href": "Lab4-Clasif.html#regresión-logística",
    "title": "4  Lab4 Clasificación",
    "section": "4.1 Regresión logística",
    "text": "4.1 Regresión logística\nVamos a ajustar un modelo de regresión logística para predecir la direction utilizando desde lagone hasta lagfive y volume. La función glm() se puede utilizar para ajustar muchos tipos de modelos lineales generalizados, incluida la regresión logística. La sintaxis de la función glm() es similar a la de lm(), pero ahora debemos pasar el argumento family = binomial para decirle a R que ejecute una regresión logística en lugar de otro tipo de modelo lineal generalizado.\n\nglm.fits <-\n  glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n      data = Smarket,\n      family = binomial)\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nNingún p-valor es significativo. El más pequeño aquí está asociado con lagone. El coeficiente negativo de este predictor sugiere que si el mercado tuvo un rendimiento positivo ayer, es menos probable que suba hoy. Sin embargo, con ese p-valor de 0.15, no hay una clara evidencia de asociación real entre lagone y direction.\nSe puede usar la función coef() para acceder solo a los coeficientes de este modelo ajustado. La función summary() permite acceder a aspectos particulares del modelo ajustado, como los p-valores para los coeficientes:\n\ncoef(glm.fits)\nsummary(glm.fits)$coef\nsummary(glm.fits)$coef[, 4] # sólo p-valores\n\n\nNota Víctor: No se muestran los resultados ni se dan más detalles… Por “no perder tiempo” con un modelo ajustado que “no merece la pena”.\n\n\n4.1.1 Predicción\nLa función predict() se puede utilizar para predecir la probabilidad de que el mercado suba, dados los valores de los predictores. La opción type = \"response\" le dice a R que genere probabilidades de la forma \\(P(Y=1|X)\\), en lugar de generar el logit (log odds, por defecto, type = \"link\"). Si no se proporciona ningún conjunto de datos a la función predict(), entonces se calculan las probabilidades para los datos de entrenamiento que se usaron para ajustar el modelo de regresión logística. A continuación se muestran sólo las primeras diez probabilidades (¡Ojo! con el modelo ajustado previamente -“que no merece la pena”-).\n\nglm.probs <- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\ncontrasts(Smarket$Direction)\n\n     Up\nDown  0\nUp    1\n\n\nNota: Sabemos que estos valores corresponden a la probabilidad de que el mercado suba, en lugar de que baje, porque con la función contrasts() (véase el anexo Herramientas) se ve que R ha creado una variable dummy con un 1 para Up.\nPara hacer una predicción sobre si el mercado subirá o bajará en un día en particular, debemos convertir estas probabilidades pronosticadas en etiquetas de clase, Up o Down. Los siguientes dos comandos crean un vector de predicciones de clase basado en si la probabilidad prevista de un aumento del mercado es mayor o menor que un determinado valor denominado punto de corte, en este caso 0.5 (elegido arbitrariamente)\n\nglm.pred <- rep(\"Down\", 1250)\npunto.corte <- 0.5\nglm.pred[glm.probs > punto.corte] = \"Up\"\n\nEl primer comando crea un vector de 1.250 elementos Down. La última línea transforma en Up todos los elementos para los que la probabilidad pronosticada de un aumento del mercado supera el punto de corte 0.5.\n\n\n4.1.2 Matriz de confusión\nDadas estas predicciones, la función table() se puede usar para producir una matriz de confusión para determinar cuántas observaciones se clasificaron correcta o incorrectamente. Al ingresar en table() dos vectores cualitativos, R creará una tabla 2x2 con recuentos del número de veces que ocurrió cada combinación, p.ej. pronosticó Up y el mercado aumentó, predijo Up y el mercado disminuyó, etc.\n\ntable(glm.pred, Smarket$Direction)\n\n        \nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n(145 + 507) / 1250\n\n[1] 0.5216\n\nmean(glm.pred == Smarket$Direction)\n\n[1] 0.5216\n\n\nLos elementos diagonales de la matriz de confusión indican predicciones correctas, mientras que los que están fuera de la diagonal representan predicciones incorrectas. Por lo tanto, nuestro modelo predijo correctamente que el mercado subiría en 507 días y que bajaría en 145 días, para un total de 507+145 = 652 predicciones correctas. La función mean() se puede utilizar para calcular la fracción de días en los que la predicción fue correcta. En este caso, la regresión logística predijo correctamente el movimiento del mercado el 52.2% de las veces.\nA primera vista, parece que el modelo de regresión logística funciona un poco mejor que las conjeturas aleatorias (“lanzar una moneda”). Sin embargo, este resultado es engañoso porque entrenamos y probamos el modelo en el mismo conjunto de 1250 observaciones En otras palabras, 100%-52.2%=47.8%, es la tasa de error de entrenamiento.\n\nNota Víctor: Es un muy buen ejercicio cambiar el valor del punto de corte y ver cómo cambian las prediccciones, la matriz de confusión y, por tanto, la tasa de error.\n\n\n\n4.1.3 Datos de entrenamiento y test/prueba\nComo se ha mencionado en el Capítulo de Aprendizaje Estadístico, la tasa de error de entrenamiento suele ser demasiado optimista: tiende a subestimar la tasa de error de test. Por ello se acude al enfoque de dividir los datos en los subconjuntos de entrenamiento y test/validación, que se ve en detalle en el siguiente Capítulo (Remuestreo).\nPara implementar esta estrategia, primero crearemos un vector correspondiente a las observaciones de 2001 a 2004 (datos de entrenamiento). Luego usaremos este vector para crear un conjunto de datos retenidos de observaciones de 2005, que constituyen el conjunto de datos de test/validación.\n\nNota Víctor: Al tratarse de datos de serie temporal, no tiene sentido que los datos de entrenamiento y test se escojan aleatoriamente, como es habitual.\n\n\ntrain <- (Smarket$Year < 2005)\nSmarket.2005 <- Smarket[!train, ]\nDirection.2005 <- Smarket$Direction[!train]\n\nLos elementos del vector train que corresponden a observaciones que ocurrieron antes de 2005 se establecen en TRUE, mientras que los que corresponden a observaciones en 2005 se establecen en FALSE. La notación != significa que no es igual a, por lo tanto, Smarket[!train, ] produce una submatriz de los datos del mercado de valores que contiene solo las observaciones para las cuales train es FALSE—es decir, las observaciones con fechas en 2005. (véanse más detalles en el anexo Herramientas)\nAjustamos un modelo de regresión logística usando solo el subconjunto de las observaciones que corresponden a fechas anteriores a 2005, usando el argumento subset. Posteriormente obtenemos las probabilidades pronosticadas de que el mercado de valores suba para cada uno de los días de nuestro conjunto de test, es decir, para los días de 2005.\n\nglm.fits <-\n  glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket,\n    family = binomial,\n    subset = train\n  )\nglm.probs <- predict(glm.fits, Smarket.2005,\n                     type = \"response\")\n\nTenga en cuenta que hemos entrenado y probado nuestro modelo en dos conjuntos de datos completamente separados: el entrenamiento se realizó solo con las fechas anteriores a 2005 y el test se realizó solo con las fechas de 2005. Finalmente, calculamos las predicciones para 2005, glm.pred y las comparamos con los movimientos reales del mercado durante ese período de tiempo, recogidos en Direction.2005. Se obtiene así la matriz de confusión.\n\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nNota: Como la notación != significa que no es igual a, el último comando calcula la tasa de error del conjunto de test.\nLos resultados son bastante decepcionantes: la tasa de error del conjunto de datos de test es de 52%, ¡peor que seleccionar al azar! Por supuesto, este resultado no es tan sorprendente, dado que, en general, no se esperaría poder utilizar los rendimientos de días anteriores para predecir el rendimiento futuro del mercado. (Después de todo, si fuera posible hacerlo, los autores se harían ricos en lugar de escribir un libro de texto de estadística).\nRecordemos que el modelo de regresión logística tenía p-valores muy decepcionantes asociados con todos los predictores, y que el p-valor más pequeño, pero no significativo, correspondía a lagone. Quizá eliminando las variables que parecen no ser útiles para predecir direction, podemos obtener un modelo más efectivo. Después de todo, el uso de predictores que no tienen relación con la respuesta tiende a causar un deterioro en la tasa de error del conjunto de datos de test (dado que dichos predictores provocan un aumento en la varianza sin una disminución correspondiente en el sesgo), por lo que eliminar dichos predictores puede, a su vez, generar una mejora. A continuación, reajustamos la regresión logística usando solo lagone y lagtwo, que parecían tener el mayor poder predictivo en el modelo de regresión logística original.\n\nglm.fits <- glm(\n  Direction ~ Lag1 + Lag2,\n  data = Smarket,\n  family = binomial,\n  subset = train\n)\nglm.probs <- predict(glm.fits, Smarket.2005,\n                     type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nAhora los resultados parecen ser un poco mejores: 56% de los movimientos diarios se han pronosticado correctamente. Vale la pena señalar que, en este caso, una estrategia mucho más simple: predecir que el mercado aumentará cada día, también será correcta el 56% de las veces. Por lo tanto, en términos de tasa de error general, el método de regresión logística no es mejor que el enfoque naive. Sin embargo, la matriz de confusión muestra que en los días en que la regresión logística predice un aumento en el mercado, tiene una tasa de precisión de 58%. Esto sugiere una posible estrategia comercial de comprar en los días en que el modelo predice un mercado en aumento y evitar transacciones en los días en que se pronostica una disminución. Por supuesto, habría que investigar más detenidamente si esta pequeña mejora era real o simplemente se debía a una casualidad.\nSupongamos que queremos predecir los rendimientos asociados con valores particulares de lagone y lagtwo. En particular, queremos predecir direction en un día en que lagone y lagtwo sean iguales a 1.2 y 1.1, respectivamente, y en un día en que sean iguales a 1.5 y -0.8. Hacemos esto usando la función predict().\n\npredict(glm.fits,\n        newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),\n        type = \"response\")\n\n        1         2 \n0.4791462 0.4960939"
  },
  {
    "objectID": "Lab4-Clasif.html#otros-métodos-de-clasificación",
    "href": "Lab4-Clasif.html#otros-métodos-de-clasificación",
    "title": "4  Lab4 Clasificación",
    "section": "4.2 Otros métodos de clasificación",
    "text": "4.2 Otros métodos de clasificación\n\n4.2.1 Análisis Discriminante Lineal\nRealizaremos un LDA (Linear Discriminant Analysis) con los datos de Smarket, usando la función lda(), del paquete MASS. Su sintaxis es idéntica a la de lm() y glm() excepto por la ausencia de la opción family. Ajustamos el modelo usando solo las observaciones anteriores a 2005 (datos de entrenamiento).\n\nlibrary(MASS)\n\n\nAdjuntando el paquete: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda.fit <- lda(Direction ~ Lag1 + Lag2, \n               data = Smarket,\n               subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit)\n\n\n\n\nLa salida de LDA indica que \\(\\hat\\pi_1=\\)0.492 y \\(\\hat\\pi_2=\\)0.508 (en la salida anterior: Prior probabilities of groups); es decir, el49.2`% de las observaciones de entrenamiento corresponden a días en los que el mercado bajó. También proporciona las medias del grupo; estos son el promedio de cada predictor dentro de cada clase, y LDA los utiliza como estimaciones de \\(\\mu_k\\) (véase el libro). Esto sugiere que existe una tendencia a que los rendimientos de los 2 días anteriores sean negativos en los días en que el mercado aumenta, y una tendencia a que los rendimientos de los días anteriores sean positivos en los días en que el mercado cae.\nLa salida coefficients of linear discriminants (coeficientes de discriminantes lineales) proporciona la combinación lineal de lagone y lagtwo que se utilizan para formar la regla de decisión LDA. En otras palabras, estos son los multiplicadores de los elementos de \\(X=x\\) en (4.24) (Véase la ecuación en el libro). Si \\(-0.642\\times\\) lagone \\(- 0.514 \\times\\) lagtwo es grande, entonces el clasificador LDA pronosticará un aumento del mercado, y si es pequeño, entonces el clasificador LDA pronosticará una caída del mercado.\nLa función plot() produce gráficos de los discriminantes lineales, obtenidos al calcular \\(-0.642\\times\\) lagone \\(- 0.514 \\times\\) lagtwo para cada una de las observaciones de entrenamiento. Las observaciones Up y Down se muestran por separado.\nLa función predict() devuelve una lista con tres elementos. El primer elemento, class, contiene las predicciones de LDA sobre el movimiento del mercado. El segundo elemento, posterior, es una matriz cuya \\(k\\)-ésima columna contiene la probabilidad posterior de que la observación correspondiente pertenezca a la \\(k\\)-ésima clase, calculada a partir de (4.15). Finalmente, x contiene los discriminantes lineales, descritos anteriormente.\n\nlda.pred <- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\n\n4.2.1.1 LDA vs reg.log\nComo se puede observar las predicciones de LDA son idénticas en este caso a las de regresión logística que obteníamos antes.\n\nlda.class <- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\n\nAplicando un umbral de 50% a las probabilidades posteriores nos permite recrear las predicciones contenidas en lda.pred$class.\n\nsum(lda.pred$posterior[, 1] >= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] < .5)\n\n[1] 182\n\n\nObserve que la salida de probabilidad posterior del modelo corresponde a la probabilidad de que el mercado disminuya:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nSe puede usar un umbral de probabilidad posterior distinto de 50% para hacer predicciones. Por ejemplo, supongamos que deseamos predecir una disminución del mercado solo si estamos muy seguros de que el mercado de hecho disminuirá ese día; digamos, si la probabilidad posterior es de al menos 90%.\n\nsum(lda.pred$posterior[, 1] > .9)\n\n[1] 0\n\n\n¡Ningún día en 2005 alcanza ese umbral! De hecho, la mayor probabilidad posterior de disminución en todo 2005 fue de 0.5202.\n\n\n\n4.2.2 Análisis Discriminante Cuadrático\nAjustaremos un modelo QDA (Quadratic Discriminant Analysis) con los datos de Smarket, mediante la función qda(), del paquete MASS. La sintaxis es idéntica a la de lda().\n\nqda.fit <- qda(Direction ~ Lag1 + Lag2, \n               data = Smarket,\n               subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nLa salida contiene las medias del grupo. Pero no contiene los coeficientes de los discriminantes lineales, porque el clasificador QDA implica una función cuadrática, en lugar de lineal, de los predictores. La función predict() funciona exactamente de la misma manera que para LDA.\n\nqda.class <- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\nEn este caso, las predicciones de QDA son precisas casi el 60% de las veces, a pesar de que los datos de 2005 no se usaron para ajustar el modelo. Este nivel de precisión es bastante impresionante para los datos del mercado de valores, que se sabe que son bastante difíciles de modelar con precisión. Esto sugiere que la forma cuadrática asumida por QDA puede capturar la verdadera relación con mayor precisión que las formas lineales asumidas por LDA y la regresión logística. Sin embargo, recomendamos evaluar el rendimiento de este método en un conjunto de test más grande antes de apostar a que este enfoque tendrá siempre más éxito.\n\n\n4.2.3 Naive Bayes\nEl modelo Naive Bayes se implementa en R mediante la función naiveBayes(), del paquete e1071. La sintaxis es idéntica a la de lda() y qda(). Por defecto, esta implementación del clasificador naive Bayes modela cada característica cuantitativa mediante una distribución gaussiana. Sin embargo, también se puede utilizar un método de densidad kernel para estimar las distribuciones.\n\nlibrary(e1071)\nnb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, \n                     data = Smarket,\n                     subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nLa salida contiene la media estimada ([,1]) y la desviación estándar ([,2]) para cada variable en cada clase. Por ejemplo, la media de lagone es 0.0428 para Direction = Down y la desviación estándar es 1.2274. Podemos comprobarlo fácilmente:\n\nmean(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nLa función predict() es sencilla.\n\nnb.class <- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nNaive Bayes funciona muy bien con estos datos, con predicciones precisas más del 59% de las veces. Esto es ligeramente peor que QDA, pero mucho mejor que LDA.\nLa función predict() también puede generar estimaciones de la probabilidad de que cada observación pertenezca a una clase en particular.\n\nnb.preds <- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5,]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110\n\n\n\n\n4.2.4 \\(K\\)-Nearest Neighbors\nRealizaremos KNN (K vecinos más próximos) usando la función knn(), del paquete class. Esta función es bastante diferente a las otras funciones de ajuste de modelos que hemos encontrado hasta ahora. En lugar de un enfoque de dos pasos en el que primero ajustamos el modelo y luego usamos el modelo para hacer predicciones, knn() forma predicciones usando un solo comando. La función requiere cuatro entradas.\n\nUna matriz que contiene los predictores asociados con los datos de entrenamiento, denominada train.X a continuación.\nUna matriz que contiene los predictores asociados con los datos para los que deseamos hacer predicciones, denominada test.X a continuación.\nUn vector que contiene las etiquetas de clase para las observaciones de entrenamiento, denominado train.Direction a continuación.\nUn valor para \\(K\\), el número de vecinos más próximos que utilizará el clasificador.\n\nUsamos la función cbind(), abreviatura de column bind, para unir las variables lagone y lagtwo en dos matrices, una para el conjunto de entrenamiento y la otra para el conjunto de validación.\n\nlibrary(class)\ntrain.X <- cbind(Smarket$Lag1, Smarket$Lag2)[train, ]\ntest.X <- cbind(Smarket$Lag1, Smarket$Lag2)[!train, ]\ntrain.Direction <- Smarket$Direction[train]\n\nLa función knn() se puede usar para predecir el movimiento del mercado para las fechas de 2005. Establecemos una semilla aleatoria antes de aplicar knn() porque si varias observaciones están empatadas como vecinos más próximos, entonces R romperá el empate al azar. Por lo tanto, se debe establecer una semilla para garantizar la reproducibilidad de los resultados.\n\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Direction, k = 1)\ntable(knn.pred, Direction.2005)\n\n        Direction.2005\nknn.pred Down Up\n    Down   43 58\n    Up     68 83\n\nmean(knn.pred == Direction.2005)\n\n[1] 0.5\n\n\nLos resultados usando \\(K=1\\) no son muy buenos, ya que solo se predice correctamente el 50% de las observaciones. Por supuesto, puede ser que \\(K=1\\) resulte en un ajuste demasiado flexible a los datos. A continuación, repetimos el análisis usando \\(K=3\\).\n\nknn.pred <- knn(train.X, test.X, train.Direction, k = 3)\ntable(knn.pred, Direction.2005)\n\n        Direction.2005\nknn.pred Down Up\n    Down   48 54\n    Up     63 87\n\nmean(knn.pred == Direction.2005)\n\n[1] 0.5357143\n\n\nLos resultados han mejorado ligeramente. Pero aumentar más el valor de \\(K\\) no proporciona más mejoras.\nParece que para estos datos, QDA proporciona los mejores resultados de los métodos que hemos examinado hasta ahora.\n\n4.2.4.1 Datos Caravan\nKNN no funciona bien en los datos de Smarket pero a menudo proporciona resultados impresionantes. Como ejemplo, aplicaremos el enfoque KNN al conjunto de datos Caravan, del paquete ISLR2. Este conjunto de datos incluye 85 predictores que miden características demográficas de 5.822 individuos. La variable respuesta es Purchase, que indica si un individuo determinado compra o no una póliza de seguro de caravana. En este conjunto de datos, solo 6% de las personas compraron un seguro de caravana.\n\ndim(Caravan)\n\n[1] 5822   86\n\nsummary(Caravan$Purchase)\n\n  No  Yes \n5474  348 \n\n348 / 5822\n\n[1] 0.05977327\n\n\n\n\n4.2.4.2 Escalado de variables\nDebido a que el clasificador KNN predice la clase de una observación dada del conjunto de validación al identificar las observaciones más próximas a ella, la escala de las variables es importante. Las variables con una escala grande tendrán un efecto mucho mayor en la distancia entre las observaciones, y por lo tanto en el clasificador KNN, que las variables con una escala pequeña.\nUna buena forma de manejar este problema es ajustar los datos de modo que todas las variables tengan media 0 y desviación estándar 1. Entonces todas las variables estarán en una escala comparable. La función scale() hace precisamente esto (véase el anexo Herramientas).\nEstandarizamos las variables de Caravan, excepto la columna 86, porque esa es la variable cualitativa Purchase.\n\nstandardized.X <- scale(Caravan[, -86])\nsd(Caravan[, 1])\n\n[1] 12.84671\n\nsd(Caravan[, 2])\n\n[1] 0.4058421\n\nsd(standardized.X[, 1])\n\n[1] 1\n\nsd(standardized.X[, 2])\n\n[1] 1\n\n\nAhora cada columna de standardized.X tiene desviación estándar 1 y media 0.\nPasamos a dividir las observaciones en un conjunto de test, que contiene las primeras 1000 observaciones, y un conjunto de entrenamiento, que contiene las observaciones restantes. Es decir, seleccionamos los conjuntos usando su índice no hay aquí una selección aleatoria (véase el anexo Herramientas)\nAjustamos un modelo KNN en los datos de entrenamiento usando \\(K=1\\) y evaluamos su desempeño en los datos de test.\n\ntest <- 1:1000\ntest.X <- standardized.X[test, ]\ntest.Y <- Caravan$Purchase[test]\ntrain.X <- standardized.X[-test, ]\ntrain.Y <- Caravan$Purchase[-test]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n\n[1] 0.118\n\nmean(test.Y != \"No\")\n\n[1] 0.059\n\n\nLa tasa de error de KNN en las 1000 observaciones para test es un poco menos de 12%. A primera vista, esto puede parecer bastante bueno. Sin embargo, dado que solo 6% de los clientes compraron un seguro, podríamos reducir la tasa de error a 6% prediciendo siempre No (última línea del chunk anterior) ¡independientemente de los valores de los predictores!\nSuponga que hay algún costo no trivial al tratar de vender un seguro a un individuo determinado. Por ejemplo, tal vez un vendedor deba visitar a cada cliente potencial. Si la empresa trata de vender seguros a una selección aleatoria de clientes, entonces la tasa de éxito será de solo 6%, lo que puede ser demasiado bajo dados los costos involucrados. En cambio, a la compañía le gustaría tratar de vender seguros solo a los clientes que probablemente lo compren. Por lo tanto, la tasa de error general no es de interés. En cambio, la fracción de individuos que se predice correctamente que comprará un seguro es de interés.\n\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nResulta que KNN con \\(K=1\\) funciona mucho mejor que seleccionar al azar entre los clientes que se prevé que comprarán un seguro. Entre 77 de dichos clientes, 9 (un 11.7%) realmente comprarán seguros. Esto es el doble de la tasa que se obtendría de seleccionar al azar.\n¿Y para otros valores de \\(K\\)?\n\nknn.pred <- knn(train.X, test.X, train.Y, k = 3)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  920  54\n     Yes  21   5\n\n5 / 26\n\n[1] 0.1923077\n\nknn.pred <- knn(train.X, test.X, train.Y, k = 5)\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  930  55\n     Yes  11   4\n\n4 / 15\n\n[1] 0.2666667\n\n\nUsando \\(K=3\\), la tasa de éxito aumenta a 19%, y con \\(K=5\\) la tasa es de 26,7%. Esto es más de cuatro veces la tasa que resulta de seleccionar al azar. ¡Parece que KNN está encontrando algunos patrones reales en un conjunto de datos difícil!\nSin embargo, aunque esta estrategia es rentable, vale la pena señalar que se prevé que solo 15 clientes comprarán un seguro usando KNN con \\(K=5\\). En la práctica, la compañía de seguros puede desear gastar recursos para convencer a más de 15 clientes potenciales para que compren un seguro.\n\n\n4.2.4.3 KNN vs. reg.log\nComo comparación, también podemos ajustar un modelo de regresión logística a los datos. Si usamos 0.5 como el límite de probabilidad pronosticado para el clasificador, entonces tenemos un problema: se predice que solo siete de las observaciones para validación comprarán un seguro, pero es peor, ¡nos equivocamos totalmente! Porque ninguno compra.\n\nglm.fits <- glm(Purchase ~ .,\n                data = Caravan,\n                family = binomial,\n                subset = -test)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm.probs <- predict(glm.fits, Caravan[test, ],\n                     type = \"response\")\nglm.pred <- rep(\"No\", 1000)\nglm.pred[glm.probs > .5] <- \"Yes\"\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  934  59\n     Yes   7   0\n\n\nSin embargo, no estamos obligados a utilizar un corte de 0.5. Si, en cambio, predecimos una compra cada vez que la probabilidad de compra predicha supere los 0.25, obtenemos resultados mucho mejores: predecimos que 33 personas comprarán un seguro y estamos en lo correcto para aproximadamente un 33% de estas personas. ¡Esto es más de cinco veces mejor que seleccionar al azar!\n\nglm.pred <- rep(\"No\", 1000)\nglm.pred[glm.probs > .25] <- \"Yes\"\ntable(glm.pred, test.Y)\n\n        test.Y\nglm.pred  No Yes\n     No  919  48\n     Yes  22  11\n\n11 / (22 + 11)\n\n[1] 0.3333333"
  },
  {
    "objectID": "Lab4-Clasif.html#regresión-de-poisson-omitido",
    "href": "Lab4-Clasif.html#regresión-de-poisson-omitido",
    "title": "4  Lab4 Clasificación",
    "section": "Regresión de Poisson (omitido)",
    "text": "Regresión de Poisson (omitido)\n\nNota Víctor: Como se ha comentado al principio, en el material original también se incluye en este Lab Regresión de Poisson que para mí no encaja en “métodos de clasificación”. Además el enfoque que se da, utilizando primero la función lm() es enrevesado (opinión personal). Por ambas cosas se omiten aquí los detalles, pero se mantienen las ideas generales de este tipo de regresión (también habitual en la práctica).\n\n\nBikeshare (en el paquete ISLR2) es el conjunto de datos elegido para ajustar un modelo de regresión de Poisson. Mide el número de alquileres de bicicletas (bikers) por hora en Washington, DC.\nPara ajustar un modelo de regresión de Poisson se usa la función glm() con el argumento family = poisson\nUna vez más podemos usar la función predict() para obtener los valores ajustados (predicciones) del modelo de regresión de Poisson. Sin embargo, debemos usar el argumento type = \"response\" para especificar que queremos que R genere \\(\\exp(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots +\\hat\\beta_p X_p)\\) en lugar de que \\(\\hat\\beta_0 + \\hat\\beta_1 X_1+ \\ldots + \\hat\\beta_p X_p\\), que generará de forma predeterminada.\nEn esta sección, se menciona el uso de la función glm() con el argumento family = poisson para realizar la regresión de Poisson. Anteriormente en esta práctica, usamos la función glm() con family = binomial para realizar una regresión logística. Se pueden usar otras opciones para el argumento family para ajustar otros tipos de GLM. Por ejemplo, family = Gamma ajusta un modelo de regresión gamma."
  }
]